{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> $k$-means Clustering </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "In this lab we are going to discuss $k$-means clustering. We will see how you can use $k$-means clustering for color quantization. By the end of this lab, you should:\n",
    "- Be able to describe the $k$-means algorithm and implement it yourself\n",
    "- Know how to use scikit-learn to perform $k$-means clustering\n",
    "- Be able to describe the difference between online $k$-means and the usual $k$-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 $k$-means\n",
    "Recall that the idea behind clustering is that you have data points $x_1,\\dots, x_n\\in \\mathbb{R}^d$ and want to group \"similar\" points together. There are no labels for the $x_j$'s, so this is an \"unsupervised learning\" task. It can be useful for exploratory data analysis.\n",
    "\n",
    "In $k$-means, the idea is that we want to group the data into $k$ clusters $C_1,\\dots, C_k$, such that the sum of the squared distances of each point to the mean of its group is minimized. I.e., we want to minimize\n",
    "$$ J(C_1,\\dots, C_k) = \\sum_{i=1}^k \\sum_{x\\in C_i} \\|x-m_i\\|^2_2,$$\n",
    "where $m_i$ is the mean of cluster $C_i$.\n",
    "\n",
    "The problem with the above objective function is that it is non-convex, and minimizing it is NP-hard. The $k$-means algorithm, described next, is guaranteed to converge to a stationary point, but it may not be the global minimum. \n",
    "\n",
    "The $k$-means algorithm works iteratively as follows:\n",
    "- **Input**: Data points $x_1,\\dots, x_n\\in \\mathbb{R}^d$ and number of clusters $k$\n",
    "- Randomly partition the data into $k$ clusters\n",
    "- While not converged:\n",
    "    1. Compute the mean of each cluster: $m_i = \\frac{1}{n_i} \\sum_{x\\in C_i} x$, where $n_i$ is the number of items in cluster $C_i$\n",
    "    2. For each point $x_j$, find the nearest cluster: $\\pi(x) = \\arg\\min_{1\\leq i\\leq k} \\|x-m_i\\|^2_2$\n",
    "    3. Update the clusters according to step (2): $C_i = \\{x: \\pi(x) = i\\}$\n",
    "- **Output**: Clusters $C_1,\\dots, C_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 $k$-means on a simple example\n",
    "Consider the following data:\n",
    "$ x_1 = (0, 1), x_2 = (0, 2), x_3 = (10, 1), x_4 = (10, 3)$. Let's plot these points:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "X = np.array([[0, 1], [0, 2], [10, 1], [10, 3]])\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlim(-1, 12)\n",
    "plt.ylim(-1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are two clusters, each with two points. Let's try performing $k$-means by hand. \n",
    "\n",
    "First we need to randomly partition the data points into clusters. For the purposes of this illustration, let's say $C_1 = \\{(0,1), (10,3)\\}$ and $C_2 = \\{(0,2), (10,1)\\}$. Here's how the $k$-means algorithm proceeds:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Compute the mean of each cluster.\n",
    "\n",
    "** Exercise 1 ** Compute the mean of each cluster. Then fill in the line in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The picture at the moment looks like the following, where the circles are our data points and the $x$'s are the cluster centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=['red', 'blue', 'blue', 'red'])\n",
    "centers = np.array([[0, 0], [0, 0]])  # TO COMPLETE: Fill in the cluster centers here \n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', c='black')\n",
    "plt.xlim(-1, 12)\n",
    "plt.ylim(-1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Compute the distance from each cluster mean to each point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 2**: Perform step 2.\n",
    "- Distance from $(0,1)$ to $m_1$: \n",
    "- Distance from $(0,1)$ to $m_2$: \n",
    "- Distance from $(10,3)$ to $m_1$: \n",
    "- Distance from $(10,3)$ to $m_2$: \n",
    "- Distance from $(0,2)$ to $m_1$: \n",
    "- Distance from $(0,2)$ to $m_2$: \n",
    "- Distance from $(10,1)$ to $m_1$: \n",
    "- Distance from $(10,1)$ to $m_2$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Update the clusters by assigning each point to the nearest cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 3 ** Perform step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_1 = \\{(?,?), (?,?) \\}$, $C_2 = \\{(?,?), (?,?) \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute the mean of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 4 ** Perform step 4. Then fill in the line in the code below.\n",
    "\\begin{align*}\n",
    "m_1 &= (?, ?)\\\\\n",
    "m_2 &= (?, ?)\n",
    "\\end{align*}\n",
    "\n",
    "Now the picture looks like the following, where the circles are our data points and the $x$'s are the cluster centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=['red', 'blue', 'red', 'blue'])\n",
    "centers = np.array([[0, 0], [0, 0]])  # TO COMPLETE: Fill in the cluster centers here \n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', c='black')\n",
    "plt.xlim(-1, 12)\n",
    "plt.ylim(-1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Find the distance from each cluster mean to each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 5 ** Perform step 5.\n",
    "- Distance from $(0,1)$ to $m_1$: \n",
    "- Distance from $(0,1)$ to $m_2$: \n",
    "- Distance from $(10,3)$ to $m_1$: \n",
    "- Distance from $(10,3)$ to $m_2$: \n",
    "- Distance from $(0,2)$ to $m_1$: \n",
    "- Distance from $(0,2)$ to $m_2$: \n",
    "- Distance from $(10,1)$ to $m_1$: \n",
    "- Distance from $(10,1)$ to $m_2$: \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Update the clusters by assigning each point to the nearest cluster.\n",
    "\n",
    "** Exercise 6 ** Perform step 6.\n",
    "$C_1 = \\{(?,?), (?,?) \\}$, $C_2 = \\{(?,?), (?,?) \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Compute the mean of each cluster.\n",
    "    \n",
    "** Exercise 7 ** Perform step 7.\n",
    "\\begin{align*}\n",
    "m_1 &= (?, ?)\\\\\n",
    "m_2 &= (?, ?)\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the clusters and means are the same as last time. Therefore, the algorithm has converged and our final clusters are $C_1 = \\{(10,3), (0,2) \\}$, $C_2 = \\{(0,1), (10,1) \\}$. This is not how we would have intuitively created our clusters. \n",
    "\n",
    "With this clustering, the value of the $k$-means objective is \n",
    "\\begin{align*}\n",
    "J(C_1,C_2) &= \\|(10,3)-(5, 2.5)\\|^2+\\|(0,2)-(5, 2.5)\\|^2+\\|(0,1)-(5, 1)\\|^2+\\|(10,1)-(5, 1)\\|^2\\\\\n",
    "&= (5^2+0.5^2) + (5^2+0.5^2) + (5^2 + 0^2) + (5^2+0^2) \\\\\n",
    "&= 100.5\n",
    "\\end{align*}\n",
    "\n",
    "Let's plot the results and then check them with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=['red', 'blue', 'red', 'blue'])\n",
    "centers = np.array([[5, 2.5], [5, 1]])\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', c='black')\n",
    "plt.xlim(-1, 12)\n",
    "plt.ylim(-1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "# We need to specify the number of clusters. I'm also inputting the initial cluster centers.\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, init=np.array([[5, 2], [5, 1.5]]))\n",
    "# Perform the clustering\n",
    "kmeans.fit(X)\n",
    "print('Cluster centers:\\n', kmeans.cluster_centers_)\n",
    "print('Objective function value:', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 $k$-means++\n",
    "If that was the end of the story, we might dismiss $k$-means as a clustering algorithm that's not very useful. Is there a way to improve the results, so that they align more with our intuition? Did the $k$-means algorithm find the global minimum in that example, or did it stop at a local minimum?\n",
    "\n",
    "It turns out that the $k$-means algorithm did not find the global minimum in that example, which illustrates the problem of trying to minimize non-convex objectives. Instead of arbitrarily choosing the initial clusters, let's do something more sophisticated. This time, we'll do the following:\n",
    "1. Choose one cluster center at random from the data points.\n",
    "2. Compute the distance $D(x)$ from each data point $x$ to the nearest cluster that has already been chosen.\n",
    "3. Select data point $x$ as the next cluster center with probability $\\frac{D(x)^2}{\\sum_y D(y)^2}$\n",
    "4. Return to 2 and continue until $k$ cluster centers have been chosen.\n",
    "\n",
    "$k$-means with this initialization method is called $k$-means++. Note that there is a random component to this initialization. Therefore, it can be beneficial to run $k$-means++ several times and then take the best clustering you get. There is some theory that limits how far a $k$-means++ clustering will be from the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try $k$-means++ on the example from above. First we need to randomly pick a cluster center from the data points. We can do this with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Set the seed so everyone gets the same results\n",
    "np.random.choice(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 8 ** Perform steps 2 and 3 above by hand. Then fill in the probabilities in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly chose the first point, $(0, 1)$ as the first center. Now we need to compute the distance from every other point to this point:\n",
    "- Distance from $(0, 2)$ to $(0, 1)$: \n",
    "- Distance from $(10, 1)$ to $(0, 1)$: \n",
    "- Distance from $(10, 3)$ to $(0, 1)$:\n",
    "\n",
    "Therefore, we will select $(0, 2)$ as the other cluster center with probability ??. Similarly, we will select $(10, 1)$ as the other cluster center with probability ?? and $(10, 3)$ as the other cluster center with probability ??. Let's randomly choose the next center according to these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(3, p=[0.33, 0.33, 0.34])  # TO COMPLETE: Change the probabilities to the correct values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy chose the last point, $(10, 3)$, as the next cluster center (Note that the function above will return a random integer between 0 and 2). Therefore, our two initial cluster centers will be $(0,1)$ and $(10,3)$. Now let's run $k$-means by hand and see what we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the distance from each cluster mean to each point:\n",
    "    - Distance from $(0,1)$ to $m_1$: $\\sqrt{(0-0)^2+(1-1)^2} = \\sqrt{0}$\n",
    "    - Distance from $(0,1)$ to $m_2$: $\\sqrt{(10-0)^2+(3-1)^2} = \\sqrt{104}$\n",
    "    - Distance from $(10,3)$ to $m_1$: $\\sqrt{(0-10)^2+(1-3)^2} = \\sqrt{104}$\n",
    "    - Distance from $(10,3)$ to $m_2$: $\\sqrt{(10-10)^2+(3-3)^2} = \\sqrt{0}$\n",
    "    - Distance from $(0,2)$ to $m_1$: $\\sqrt{(0-0)^2+(1-2)^2} = \\sqrt{1}$\n",
    "    - Distance from $(0,2)$ to $m_2$: $\\sqrt{(10-0)^2+(3-2)^2} = \\sqrt{101}$\n",
    "    - Distance from $(10,1)$ to $m_1$: $\\sqrt{(0-10)^2+(1-1)^2} = \\sqrt{100}$\n",
    "    - Distance from $(10,1)$ to $m_2$: $\\sqrt{(10-10)^2+(3-1)^2} = \\sqrt{4}$\n",
    "2. Update the clusters by assigning each point to the nearest cluster:\n",
    "$C_1 = \\{(0,1), (0,2) \\}$, $C_2 = \\{(10,3), (10,1) \\}$\n",
    "3. Compute the mean of each cluster:\n",
    "\\begin{align*}\n",
    "m_1 &= \\left(\\frac{0+0}{2}, \\frac{1+2}{2}\\right) = (0, 1.5)\\\\\n",
    "m_2 &= \\left(\\frac{10+10}{2}, \\frac{3+1}{2}\\right) = (10, 2)\n",
    "\\end{align*}\n",
    "4. Find the distance from each cluster mean to each point:\n",
    "    - Distance from $(0,1)$ to $m_1$: $\\sqrt{(0-0)^2+(1.5-1)^2} = \\sqrt{0.25}$\n",
    "    - Distance from $(0,1)$ to $m_2$: $\\sqrt{(10-0)^2+(2-1)^2} = \\sqrt{101}$\n",
    "    - Distance from $(10,3)$ to $m_1$: $\\sqrt{(0-10)^2+(1.5-3)^2} = \\sqrt{102.25}$\n",
    "    - Distance from $(10,3)$ to $m_2$: $\\sqrt{(10-10)^2+(2-3)^2} = \\sqrt{1}$\n",
    "    - Distance from $(0,2)$ to $m_1$: $\\sqrt{(0-0)^2+(1.5-2)^2} = \\sqrt{0.25}$\n",
    "    - Distance from $(0,2)$ to $m_2$: $\\sqrt{(10-0)^2+(2-2)^2} = \\sqrt{100}$\n",
    "    - Distance from $(10,1)$ to $m_1$: $\\sqrt{(0-10)^2+(1.5-1)^2} = \\sqrt{100.25}$\n",
    "    - Distance from $(10,1)$ to $m_2$: $\\sqrt{(10-10)^2+(2-1)^2} = \\sqrt{1}$\n",
    "5. Update the clusters by assigning each point to the nearest cluster:\n",
    "$C_1 = \\{(0,1), (0,2) \\}$, $C_2 = \\{(10,3), (10,1) \\}$\n",
    "6. Compute the mean of each cluster:\n",
    "\\begin{align*}\n",
    "m_1 &= \\left(\\frac{0+0}{2}, \\frac{1+2}{2}\\right) = (0, 1.5)\\\\\n",
    "m_2 &= \\left(\\frac{10+10}{2}, \\frac{3+1}{2}\\right) = (10, 2)\n",
    "\\end{align*}\n",
    "\n",
    "At this point, the clusters and means are the same as last time. Therefore, the algorithm has converged and our final clusters are $C_1 = \\{(0,1), (0,2) \\}$, $C_2 = \\{(10,3), (10,1) \\}$. This *is* how we would have intuitively created our clusters. \n",
    "\n",
    "With this clustering, the value of the $k$-means objective is \n",
    "\\begin{align*}\n",
    "J(C_1,C_2) &= \\|(10,3)-(10, 2)\\|^2+\\|(0,2)-(0, 1.5)\\|^2+\\|(0,1)-(0, 1.5)\\|^2+\\|(10,1)-(10, 2)\\|^2\\\\\n",
    "&= (0^2+1^2) + (0^2+0.5^2) + (0^2 + 0.5^2) + (0^2+1^2) \\\\\n",
    "&= 2.5,\n",
    "\\end{align*}\n",
    "suggesting this clustering is much better than the clustering we got the first time.\n",
    "\n",
    "Let's plot the results and then check them with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=['red', 'red', 'blue', 'blue'])\n",
    "centers = np.array([[0, 1.5], [10, 2]])\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', c='black')\n",
    "plt.xlim(-1, 12)\n",
    "plt.ylim(-1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify the number of clusters. I'm also inputting the initial cluster centers.\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, init=np.array([[0, 1], [10, 3]]))\n",
    "# Perform the clustering\n",
    "kmeans.fit(X)\n",
    "print('Cluster centers:\\n', kmeans.cluster_centers_)\n",
    "print('Objective function value:', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sklearn's $k$-means implementation has an option to use $k$-means++:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify 'k-means++' for the init parameter.\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, init='k-means++')\n",
    "# Perform the clustering\n",
    "kmeans.fit(X)\n",
    "print('Cluster centers:\\n', kmeans.cluster_centers_)\n",
    "print('Objective function value:', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, sklearn runs $k$-means 10 times, randomly initializing acccording to the $k$-means++ algorithm each time. It then outputs the clustering with the best objective function value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 $k$-means for color quantization\n",
    "Now that we know how $k$-means works, let's try it out on an application: color quantization. A color image is made up of pixels that each have a red, green, and blue value. Consequently, an image might have 100,000 unique colors. If you have a lot of images and you want to store all of the colors in every image, the amount of storage space required will be huge. The idea behind color quantization is to represent images using fewer unique colors, such that the new images look as much like the original images as possible. One method for finding which colors to use in the new representations is to use $k$-means clustering. Let's try it.\n",
    "\n",
    "This example is slightly modified from \n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "\n",
    "n_colors = 64  # Number of cluster centers/colors we will use to represent the image\n",
    "\n",
    "# Load a picture of the Summer Palace\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "\n",
    "# The image is 427 (width) by 640 (height) by 3 (for RGB color channels)\n",
    "print('Shape of image:', china.shape)\n",
    "\n",
    "# Let's look at the first pixel. \n",
    "print('First pixel:', china[0, 0, :])\n",
    "\n",
    "# Each pixel value can range from 0 to 255:\n",
    "print('Smallest pixel value:', np.min(china))\n",
    "print('Largest pixel value:', np.max(china))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to floats instead of the default 8 bits integer coding. Dividing by\n",
    "# 255 is important so that plt.imshow behaves works well on float data (need to\n",
    "# be in the range [0-1])\n",
    "china = np.array(china, dtype=np.float64) / 255\n",
    "\n",
    "# Let's take a look at the image\n",
    "plt.imshow(china)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and transform to a 2D numpy array, where each row is for one pixel\n",
    "w, h, d = original_shape = tuple(china.shape)\n",
    "assert d == 3\n",
    "image_array = np.reshape(china, (w * h, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running $k$-means on all $427\\times 640 = 273,280$ pixels would take a while. Let's therefore take a sample of the pixels instead and run $k$-means on just these pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting a small sub-sample of the data\")\n",
    "image_array_sample = shuffle(image_array, random_state=0)[:1000]  # Take a random subsample of size 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 9 ** Use Scikit-learn to fit a k-means model on this subsample with `n_colors = 64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code just ran $k$-means on the pixels. In other words, it clustered the pixel colors into 64 different groups. What colors did it choose? We can look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First recall that each cluster center is just a vector of three numbers:\n",
    "kmeans.cluster_centers_[0, :]\n",
    "\n",
    "# Now let's plot the colors that were the cluster centers in k-means.\n",
    "colors = np.expand_dims(kmeans.cluster_centers_, axis=1)\n",
    "plt.axis('off')\n",
    "plt.imshow(colors, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign each pixel in the entire image (not just the pixels we ran $k$-means on) to the nearest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels (nearest cluster means) for all points\n",
    "print(\"Predicting color indices on the full image using results from k-means\")\n",
    "t0 = time()\n",
    "labels = kmeans.predict(image_array)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the image looks like when we recreate it using our $k$-means result. In the recreation, the color of each pixel is going to be the color of the nearest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d))\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i][j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, K-Means)')\n",
    "plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder what would happen if we just chose 64 random colors rather than 64 colors resulting from $k$-means. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_random = shuffle(image_array, random_state=0)[:n_colors]\n",
    "print(\"Predicting color indices on the full image (random)\")\n",
    "t0 = time()\n",
    "labels_random = pairwise_distances_argmin(codebook_random, image_array, axis=0)\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a side-by-side display of the original image and the two quantized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all results, alongside original image\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Original image (96,615 colors)')\n",
    "plt.imshow(china)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, k-means)')\n",
    "plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n",
    "\n",
    "plt.figure(3)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, random)')\n",
    "plt.imshow(recreate_image(codebook_random, labels_random, w, h))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Online $k$-means\n",
    "In certain applications your data may be \"streaming\", meaning you are continuously collecting data. Therefore, if you want to run $k$-means on all of the data at once, this isn't possible. Instead, we can update the cluster centers as new points come in. In online $k$-means, we sample one point at a time and find the nearest cluster center. We then update the cluster mean and a count for each cluster. In other words, the algorithm is as follows:\n",
    "\n",
    "- **Input**: Data points $x_1,\\dots, x_n\\in \\mathbb{R}^d$ and number of clusters $k$\n",
    "- Initialize means $m_i$. Set $n_i=0$, for $i=1,2,\\dots, k$. \n",
    "- While not converged:\n",
    "    1. Choose a data point $x$ and determine it's cluster assignment: $\\pi(x) = \\arg\\min_{1\\leq i\\leq k} \\|x-m_i\\|^2_2$ (i.e., find the nearest cluster center.)\n",
    "    3. Update the mean of the cluster $x$ was assigned to: $n_{\\pi(x)} \\leftarrow n_{\\pi(x)}+1$ and $m_{\\pi(x)} \\leftarrow m_{\\pi(x)} + \\frac{1}{n_{\\pi(x)}} (x-m_{\\pi(x)})$\n",
    "- **Output**: Clusters centers $m_1,\\dots, m_k$\n",
    "\n",
    "You should check that the update of the cluster means is what you would expect. Let's try applying this on the simple example we saw at the beginning of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, assume that I shuffled the data $x_1=(0,1), x_2=(0,2), x_3=(10,1), x_4=(10,3)$ and after shuffling, the order was $x_2, x_1, x_3, x_4$. I will take the first two points as the initialization of the means, so $m_1=(0,2), m_2=(0,1)$. Since we used the points themselves as the means, we'll set $n_1=n_2=1$. Now we do the following:\n",
    "- Pick the next point, $x_3$, and determine its cluster assignment. $x_3$ is nearest to $m_2$ (distance of $10$ vs. $\\sqrt{101}$). \n",
    "- Update the mean of cluster 2: $n_2\\leftarrow 2$, $m_2 \\leftarrow (0,1) + \\frac{1}{2}((10,1)-(0,1)) = (5,1)$.\n",
    "- Pick the next point, $x_4$, and determine its cluster assignment. $x_4$ is nearest to $m_2$ (distance of $\\sqrt{29}$ vs. $\\sqrt{101}$).\n",
    "- Update the mean of cluster 2: $n_2\\leftarrow 3$, $m_2 \\leftarrow (5,1) + \\frac{1}{3}((10,3)-(5,1)) = \\left(\\frac{20}{3},\\frac{5}{3}\\right)$.\n",
    "\n",
    "In this case, the data is not streaming, and so we could perform more passes through our data set. If the data was streaming, we would continue getting more points $x_5, x_6, \\dots$ and updating the means. Let's try online $k$-means on the color quantization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "def online_kmeans(data, k, num_passes):\n",
    "    \"\"\"\n",
    "    Run the online k-means algorithm (not in a streaming setting).\n",
    "    Inputs:\n",
    "    - data: Matrix of data points on which to run k-means. Each row is one data point.\n",
    "    - num_passes: Number of times to go through the entire data set.\n",
    "    Outputs:\n",
    "    - centers: Mean of each cluster.\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    row_indices = np.arange(n)\n",
    "    np.random.shuffle(row_indices)\n",
    "    cluster_centers = np.array([data[row_indices[i], :] for i in range(k)])\n",
    "    counts = np.zeros(k)\n",
    "    for i in range(num_passes):\n",
    "        for j in range(n):\n",
    "            idx = row_indices[j]\n",
    "            x = data[idx, :]\n",
    "            if i != 0 or j >= k:  # Skip the points you used to create the clusters during the first iteration\n",
    "                dists = scipy.spatial.distance.cdist(x[np.newaxis, :], cluster_centers)  # Compute distance to each cluster center\n",
    "                argmin_dist = np.argmin(dists)  # Find the nearest cluster center\n",
    "                counts[argmin_dist] += 1\n",
    "                cluster_centers[argmin_dist] += 1/counts[argmin_dist]*(x-cluster_centers[argmin_dist]) # Update mean\n",
    "        np.random.shuffle(row_indices)\n",
    "    return cluster_centers\n",
    "\n",
    "# Estimate cluster centers (colors to use). We'll use just one pass through the data.\n",
    "cluster_centers = online_kmeans(image_array, 64, 1)  \n",
    "# Find nearest center (color)\n",
    "labels_online = pairwise_distances_argmin(cluster_centers, image_array, axis=0)\n",
    "\n",
    "# Display all results, alongside original image\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Original image (96,615 colors)')\n",
    "plt.imshow(china)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, k-means)')\n",
    "plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n",
    "\n",
    "plt.figure(3)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, online k-means)')\n",
    "plt.imshow(recreate_image(cluster_centers, labels_online, w, h))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
