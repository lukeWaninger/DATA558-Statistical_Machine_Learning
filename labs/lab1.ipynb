{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Python and Scikit-learn</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "In this lab you will learn the following:\n",
    "- How to install Python and Python packages\n",
    "- How to perform basic operations in Python\n",
    "- How to load packages and use common functions from NumPy\n",
    "- How to read in and manipulate datasets with Pandas\n",
    "- How to split a dataset into training and testing sets with scikit-learn\n",
    "- How to fit nearest neighbors and ridge regression models in scikit-learn and use them to make predictions\n",
    "\n",
    "Some parts of this tutorial are adapted from https://github.com/amueller/introduction_to_ml_with_python and the corresponding book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Installing Python \n",
    "There are many ways to install Python, including:\n",
    "- With Anaconda: https://www.continuum.io/downloads\n",
    "- Directly from the Python website: https://www.python.org/downloads/\n",
    "\n",
    "It tends to be easier to use Anaconda. Note that a lot of people are still using version 2.x and not 3.x. There are differences in the syntax, and Python 3.x is not backward compatible! For this tutorial, I'll use Python 3.6, as people are increasingly switching to this version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Installing Packages\n",
    "Like in R and many other languages, you will often need to use functions not built into Python. In this tutorial we will be using numpy, scikit-learn, matplotlib, and pandas. If you are using Anaconda, they may be already installed. If not, you can install them at the command line via\n",
    "~~~\n",
    "conda install numpy scikit-learn matplotlib pandas\n",
    "~~~\n",
    "On the other hand, if you are using pip to install packages, you can do so via\n",
    "~~~\n",
    "pip install numpy scikit-learn matplotlib pandas\n",
    "~~~\n",
    "On Windows, if you don't use Anaconda you may encounter problems installing packages. In that case, there are a bunch of packages you can install from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/  This website contains Windows binaries for many packages. These files have strings like cp27 and cp36 in their names. These correspond to the Python versions. Make sure you also correctly download either the 32- or 64-bit version, depending on whether you have 32- or 64-bit python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Choosing a development environment\n",
    "Like RStudio for R, there are integrated development environments (IDEs) for Python. These make life a lot easier! For the labs we will be using Jupyter notebooks. If you are using Anaconda you don't need to install anything extra. If you need to install it at the command line, try running\n",
    "~~~\n",
    "pip3 install jupyter\n",
    "~~~\n",
    "To start a Jupyter notebook, go to the command line and type \n",
    "~~~\n",
    "jupyter notebook\n",
    "~~~\n",
    "\n",
    "While Jupyter notebooks are great for class, they're not as convenient when writing large quantities of code. This Stack Overflow page has a nice comparison of many IDEs, so you can choose based on your preferences: http://stackoverflow.com/questions/81584/what-ide-to-use-for-python\n",
    "PyCharm, Eclipse with PyDev, Komodo, Emacs, and Spyder seem to be some of the more popular ones.\n",
    "Features of good IDEs include automatic code completion, an integrated debugger, error markup, version control integration (http://git-scm.com/)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Basic operations and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'll see in this section, most of the operations in Python are similar to those in other languages like R and MATLAB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Arithmetic operations, lists, and dictionaries\n",
    "Some of the most important things you need to know are:\n",
    "- Python uses zero-based indexing.\n",
    "- Indentations matter.\n",
    "- The exponentiation operator is **, not ^.\n",
    "\n",
    "We will see these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't worry about these two lines. They just tell the notebook to display all of the results.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text after a pound sign is a comment.\n",
    "(2 / 8) ** 3 + 2.1 * .003  # Two asterisks denote \"to the power of\". Not ^ like in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2/8  # Unlike in Python 2, Python 3 doesn't perform integer division. If you don't know what this is, don't worry \n",
    "     # about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3  # This sets the variable x to the number 3.\n",
    "x_new = x+1  # Names can't have periods in them, unlike in R.\n",
    "x_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [10, 20, 30]  # Create a list.\n",
    "y\n",
    "y[0]  # Python uses zero-based indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [4, 5, 6]\n",
    "x2+y  # Adding two lists concatenates them. There is a separate array type we will introduce in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2[-2]  # Using a minus sign before a number gives the element that is that far from the end, unlike in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.append('hi')  # Lists can have different data types in them\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'a': 10, 'b': 'hi', 'c': [1,2,3]}  # Create a \"dictionary\"\n",
    "my_dict['c']  # Access the \"value\" corresponding to the \"key\" 'c' in the dictionary I created\n",
    "my_dict['a']  \n",
    "my_dict.values()\n",
    "my_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(0, 10, 2)  # Range creates a sequence of numbers starting at the first entry and ending BEFORE the second \n",
    "                 # entry. The third entry is the step size. The range() function is really useful when creating for \n",
    "                 # loops.\n",
    "list(range(0, 10, 2))  # Convert to a list for printing purposes\n",
    "list(range(0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll introduce `for` loops, `while` loops, and functions. At this point it's important to know that in Python it is necessary to follow indentation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-2, 5):  # Note (1) there are no parentheses; and (2) There is a colon at the end.\n",
    "    print(abs(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 2\n",
    "while i < 5 and not j >= 4:  # To check multiple conditions, use \"and\", \"or\", and/or \"not\"\n",
    "    i += 1  # This is short for i = i+1\n",
    "    j = j+1\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world(my_string, freq):  # Define the function called hello_world with input arguments my_string and freq\n",
    "    print(my_string*freq)  # String multiplication concatenates replicated strings\n",
    "    \n",
    "hello_world('Hello, world!', 2) # Call the function hello_world\n",
    "\n",
    "'hello'+'world'  # String addition concatenates strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Importing packages\n",
    "You can import a package a number of different ways in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  # Import NumPy. You will refer to it as numpy later.\n",
    "numpy.zeros(3)  # Create a numpy array with three zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy and refer to it as np later\n",
    "np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *  # Import every function from numpy. You won't need to use the name numpy later \n",
    "                     # when referring to the functions\n",
    "zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros  # Import only the zeros function\n",
    "zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random  # Import the module \"random\" that exists within the package \"numpy\"\n",
    "random.rand(3)  # Generate three random numbers from Unif(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "numpy.random.rand(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not recommend frequently using the third and fourth methods. With these methods it's not easy to tell from looking at the code where the functions you call come from. Moreover, there could be name clashes between packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy package allows you to create arrays, generate random numbers, and perform quite a few arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])  # Create a 2-dimensional array\n",
    "print(\"x:\\n{}\".format(x))  # Print it to the screen. \\n says to go to the next line\n",
    "                           # {} and .format() are used to print objects to the screen\n",
    "                           # in Python 3.\n",
    "print(\"x[1,2] =\", x[1,2])   # Alternative way of printing to the screen. Is this the entry of x you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate random numbers from a normal distribution we can use the np.random.normal() function. The first argument is the location (mean), the second argument is the scale (standard deviation), and the third argument tells it how many random numbers we want and in what format. size=(2,3) says that we want a 2x3 matrix as the output. You can also generate random numbers from many other distributions (see the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Set the seed for reproducibility\n",
    "my_sample = np.random.normal(loc=1, scale=2, size=(2, 3))\n",
    "x\n",
    "x+my_sample  # Add two arrays\n",
    "np.sum(x)  # Add all of the entries in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(x)  # Take the square root of x element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ** 2  # Square the entries in x element-wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*x  # Multiply individual elements in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.T  # Transpose of x\n",
    "print('y=', y)\n",
    "print('Dimensions of y:', y.shape)  # Dimensions of y\n",
    "print('Number of rows in y:', y.shape[0])  # Number of rows\n",
    "print('Number of rows in y:', np.size(y, 0))  # Another way to get the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.dot(x) # Matrix multiplication\n",
    "np.dot(y, x)  # Does the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1:2, :]  # Get the second row of the matrix y and every column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy has quite a few linear algebra routines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(x, x.T)\n",
    "np.linalg.inv(z)  # Inverse of z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(z)[0]  # Eigenvalues of z\n",
    "np.linalg.eig(z)[1]  # Corresponding eigenvectors of z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the labs I will try to include exercises for you, like the ones below. These are not graded, but should aid in your understanding and possibly help you directly or indirectly with the homework.\n",
    "\n",
    "**Exercise 1.** Consider the vector $x = (1, 3, 5, 7, 9)$. Using Python, compute the square root of the sum of\n",
    "the squares of its elements, i.e., $1^2 + 3^2 + 5^2 + 7^2 + 9^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Generate data using the following code:\n",
    "~~~\n",
    "np.random.seed(0)\n",
    "A = np.random.uniform(size=(4,4))\n",
    "A = A/np.linalg.norm(A, 1, axis=0)\n",
    "~~~\n",
    "Let $A_{i, j}$ denote the element in the $i^{th}$ row and $j^{th}$ column of $A$ (using 1-based indexing). Compute the following quantity without using a `for` loop:\n",
    "$$ \\frac{1}{2}\\sum_{j=1}^4 |A_{2,j}-A_{4,j}| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 A word of caution\n",
    "Sometimes Python does things you may not expect. The code below can be unexpected and if you end up with a bug related to it, it can be very difficult to find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.ones((2, 2))  # Create a matrix of ones\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B[0, 1] = 2  # Change one entry in B\n",
    "B \n",
    "A  # This also changed the same entry in A!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did that happen? When you set B to A, you are copying a memory address, not creating a new copy of A. You should keep this in mind! To avoid this problem, you can use the copy package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "A = np.ones((2, 2))  \n",
    "B = copy.copy(A)  # Copy A instead of its memory address\n",
    "B[0, 1] = 2\n",
    "B\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Find the bugs in the following lines of code:\n",
    "1. `5**1/2  # Compute the square root of 5`\n",
    "2. `range(1, 5)  # Create a list of numbers between 1 and 5, inclusive`\n",
    "3. ~~~\n",
    "import np.sqrt as sqrt\n",
    "a = 4\n",
    "sqrt = 2\n",
    "b = 5\n",
    "sqrt_b = sqrt(5)  # Compute the square root of 5\n",
    "~~~\n",
    "4. ~~~\n",
    "for i in range(5, 0):  # Loop over the values from 5 down to 0 (inclusive)\n",
    "        print(i)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you may think some of these bugs are contrived, you will likely encounter at least one of them in some (possibly more complicated) form in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Plotting\n",
    "Matplotlib is the main plotting package in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line below says to display the graphs inside this notebook\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  # Import the matplotlib plotting library\n",
    "\n",
    "# Generate a sequence of numbers from -10 to 10 with 100 steps in between\n",
    "x = np.linspace(-10, 10, 100)\n",
    "# Create a second array using sine\n",
    "y = np.sin(x)\n",
    "# The plot function plots the first argument on the x-axis, the second argument on the y-axis and \n",
    "# connects the points.\n",
    "plt.plot(x, y, marker=\"x\")\n",
    "plt.xlabel('x label')  # Add a label to the x-axis\n",
    "plt.ylabel('y label')  # Add a label to the y-axis\n",
    "plt.title('My Title')  # Add a plot title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)  # Scatter doesn't connect the points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Reading and manipulating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library Pandas is quite popular for reading in datasets and manipulating them. Below are a few things you can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a simple dataset of people\n",
    "data = {'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n",
    "        'Location' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\n",
    "        'Age' : [24, 13, 53, 33]\n",
    "       }\n",
    "\n",
    "data_pandas = pd.DataFrame(data)  # Turn the dictionary into a data frame\n",
    "# IPython.display allows \"pretty printing\" of dataframes in the Jupyter notebook\n",
    "from IPython.core.display import display\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of many possible ways to query the table:\n",
    "# selecting all rows that have an age column greater than 30\n",
    "display(data_pandas[data_pandas.Age > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(data_pandas.Age)  # Get the Age column and convert it to a numpy array\n",
    "data_pandas.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had a dataset that you wanted to load, you could use pd.read_table() or pd.read_csv(). For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the book's Auto dataset\n",
    "# sep=',' says that the data is comma-delimited\n",
    "# header=0 denotes the fact that the header is the first line of data. If there's no header, use header=None\n",
    "pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Auto.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Fitting machine learning models\n",
    "The package scikit-learn is the most popular machine learning package in Python. It allows you to easily fit a wide variety of models. Here we'll show you how to fit nearest neighbors and ridge regression models in scikit-learn and use them to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Loading data from scikit-learn\n",
    "First we'll take a look at the iris dataset in scikit-learn. Suppose that we are interested in identifying the species of some Iris plants we found. We have information on the irises' petal and septal widths and lengths. Moreover, we have the same information from a botanist on a bunch of already identified Iris plants that were labeled as being one of the species *setosa*, *versicolor*, or *virginica*. We will fit a machine learning model to the data with the known species type in order to classify our new plants.\n",
    "\n",
    "First, let's load the dataset and see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()  # Load the iris dataset from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn datasets are objects that are similar to dictionaries, in that they have keys and values. In this case, the object contains the following keys and corresponding values:\n",
    "- data: A numpy array with the measurements of the Iris plants (the predictors)\n",
    "- target: A numpy array with the type of flower for each plant (the response)\n",
    "- target_names: A list of strings, containing the three possible species of flowers \n",
    "- DESCR: A short description of the dataset\n",
    "- feature_names: A list of strings, with a description of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys of iris_dataset: {}\".format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset['DESCR'][:193] + \"\\n...\")  # Get the first part of the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target names: {}\".format(iris_dataset['target_names']))  # Names of response categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature names: {}\".format(iris_dataset['feature_names']))  # Names of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of data: {}\".format(type(iris_dataset['data'])))  # Data type of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data: {}\".format(iris_dataset['data'].shape))  # 150 samples, each with 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First five rows of data:\\n{}\".format(iris_dataset['data'][:5, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of target: {}\".format(type(iris_dataset['target'])))  # Data type of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of target: {}\".format(iris_dataset['target'].shape))  # 150 samples (matches the dimensions from above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target (response) is coded as 0-2, with the numbering in the same order as the species names appear in iris_dataset['target_names']. In other words, 0 corresponds to *setosa*, 1 corresponds to *versicolor*, and 2 corresponds to *virginica*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target:\\n{}\".format(iris_dataset['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Splitting data into training and test sets\n",
    "In order to be able to assess our model's performance we will divide the data into training and test sets. Scikit-learn provides a nice function called `train_test_split()` that will do this for you. It shuffles the data and then randomly divides it into training and test sets. The first two arguments are the features (X) and the response (y). There are a number of additional optional arguments, including `random_state` (the state for the random number generator), and `test_size` (the fraction of samples to go into the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))  # 112 samples in the training set\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape: {}\".format(X_test.shape))  # 38 samples in the test set\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Examining the data\n",
    "Before running any analysis on your data, the first thing you should do is look at it. Check if there are any errors in the data set and whether there are any outliers. Maybe your problem could be solved without machine learning or you can't answer your problem because you don't have the appropriate data. It's better to find out these things now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the data in X_train\n",
    "# Label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "# Create a scatter matrix from the dataframe and color by y_train\n",
    "# s is the marker size and alpha is the transparency\n",
    "grr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\n",
    "                        hist_kwds={'bins': 20}, s=60, alpha=.8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplots we can see that the three types of Iris seem to be well-separated in terms of their sepal and petal lengths and widths. Thus, a machine learning algorithm should be able to predict the species well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 $k$-nearest neighbors\n",
    "Let's try using a $k$-nearest neighbors model to classify the flowers. Here's how $k$-nearest neighbors works: Consider a training dataset $(x_i,y_i), i=1,\\dots, n$ of features $x_i$ and labels $y_i$ and suppose we want to classify a new point $x_{\\text{test}}$. The algorithm finds the $k$ nearest points $x_{k_1},\\dots, x_{k_k}$ to $x_{\\text{test}}$ from the training set. The predicted label for $x_{\\text{test}}$ is then the most frequent class label from $y_{k_1},\\dots, y_{k_k}$. (Here I use the indexing $k_j, j=1,\\dots, k$ rather than just $j=1,\\dots,k$ to denote the fact that the nearest points are not necessarily $x_1,\\dots, x_k$.)\n",
    "\n",
    "Consider an example: Suppose we have a training set $\\{(2, 1), (2, 1), (3, 2), (5, 1), (-1, 2)\\}$ and a new point $x_{\\text{test}}=2$ that we want to classify. Let's choose to use a 3-nearest neighbor classifier. Then the three nearest points in the training set have labels $1, 1$, and $2$. Taking the mode of those values, we predict that $x_{\\text{test}}$ should have label $1$. \n",
    "\n",
    "Note that in general the $x_i$'s don't need to be real values in $\\mathbb{R}$. In fact, we don't even need them to be in $\\mathbb{R}^d$ for some $d$! But that's for later...\n",
    "\n",
    "One nice thing about scikit-learn is that it contains many different machine learning algorithms, and the way you use them is standardized. I.e., the way you call one algorithm is similar to the way you call others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Standardizing the data\n",
    "It is often best to standardize the data before inputting it into your model. For a classifiation task we standardize the data by subtracting the mean of the predictors (the $x$'s) and dividing by their standard deviation. We can perform this using Scikit-learn's StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Fitting the model\n",
    "\n",
    "The $k$-nearest neighbors classifier is implemented in the KNeighborsClassifier class in the neighbors module. The first thing we have to do is load the classifier and instantiate it. If you don't know what classes are, don't worry about it. The important thing is knowing enough to be able to use the functions in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  # Load the classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)  # Instantiate the class by giving it n_neighbors (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knn object we just created has a bunch of methods that you might find useful. These include:\n",
    "- fit(X,y): Fit the model using X as training data and y as target values\n",
    "- get_params(): Get parameters for this estimator.\n",
    "- kneighbors([X, n_neighbors, return_distance]): Finds the n_neighbors nearest neighbors of a point and returns their distances if return_distance is set to True.\n",
    "- predict(X): Predict the class labels for the provided data\n",
    "- predict_proba(X): Return probability estimates for the test data X.\n",
    "- score(X, y[, sample_weight]): Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "The *fit* method should be the first one you use, since you need to fit the model before the model is able to do things like make predictions. In order to call one of these methods, you use knn.method_name, where method_name is the name of one of the methods from above. Aside from the kneighbors method, the others are standard across many sklearn algorithms. \n",
    "\n",
    "Let's try fitting our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)  # This modifies the knn object in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Predicting and computing the accuracy\n",
    "Now that we have fit the data we can try predicting on some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])  # One new data point, but it needs to be in a 2-d array\n",
    "print(\"X_new.shape: {}\".format(X_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict(X_new)\n",
    "print(\"Prediction: {}\".format(prediction))\n",
    "print(\"Predicted target name: {}\".format(\n",
    "       iris_dataset['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well the model performs on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(\"Test set predictions:\\n {}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))  # Manually compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))  # Use knn.score to compute the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved an accuracy of 92%! In summary, the main functions we used that you need to know are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Cross-validation over $k$\n",
    "Typically we don't know what the best choice of $k$ is. Therefore, we may choose to cross-validate over $k$. There are several ways to perform cross-validation. These include:\n",
    "1. Using a hold-out set. In this case for each choice of the parameter $k$ you fit the model to the data and compute the value of the error metric of interest (e.g., accuracy) on the hold-out set. The value of the parameter giving the lowest error on the hold-out set is then chosen.\n",
    "2. Using $n$-fold cross-validation. (Usually it's called $k$-fold cross-validation but here we already have a $k$ in $k$-nearest neighbors.) This means that the data set is divided into $n$ parts (or “folds”). Initially the first fold is treated as the validation set, and the model is fit on the rest of the data for each value of the parameter $k$. The error metric of interest (e.g., accuracy) is computed on the held-out fold for each value of $k$. The procedure is repeated $n-1$ more times so that each time a different fold is the validation set. The parameter setting that gives the best result on average may be chosen.\n",
    "\n",
    "There are two questions you may be wondering:\n",
    "1. How should you decide what cross-validation method to use?  \n",
    "If you have a very large amount of data, you can use a hold-out set. Otherwise, $n$-fold cross-valiation is better.\n",
    "\n",
    "2. How should you choose $n$?  \n",
    "The course textbook \"Elements of Statistical Learning\" recommends using 5- or 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see how to use scikit-learn's `GridSearchCV` to perform cross-validation. By default it performs 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a classifier: k-NN\n",
    "knn = KNeighborsClassifier() \n",
    "parameters = {'n_neighbors':[i for i in range(1, 11)]}\n",
    "clf = GridSearchCV(knn, parameters, cv=5)\n",
    "\n",
    "# We learn the model on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the flower types on the test set\n",
    "knn_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(np.mean(y_test == knn_predicted)*100))\n",
    "print(\"Best k:\", clf.best_params_['n_neighbors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did much better when we used cross-validation to choose $k$: 97% accuracy vs. 92% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Ridge regression\n",
    "Now let's see how to use one other machine learning algorithm in scikit-learn: ridge regression. For this part of the lab we'll use the $\\texttt{Hitters}$ dataset, which contains information on a baseball player’s\n",
    "salary (the response $y$) and various statistics related to their performance in the previous year (the predictors $x$'s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Data preparation \n",
    "First we'll load the dataset. Then we'll remove the NaNs, split it into the predictors and response, and divide it into training and test sets. We also need to convert the categorical variables into dummy variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Someone has kindly posted a bunch of datasets from R on Github. The Hitters dataset is one of them.\n",
    "hitters = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Hitters.csv', sep=',', header=0)\n",
    "hitters.head(5)  # Display the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As before, we'll remove the NaNs\n",
    "hitters = hitters.dropna() \n",
    "hitters.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our X matrix with the predictors and y vector with the response\n",
    "X = hitters.drop('Salary', axis=1) # Axis denotes either the rows (0) or the columns (1)\n",
    "X.head(3)\n",
    "y = hitters.Salary\n",
    "print('y =')\n",
    "print(y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the variables League, Division, and NewLeague\n",
    "X = pd.get_dummies(X, drop_first=True) # drop_first=True says we only want k-1 dummies out of k categorical levels\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test sets. By default, 25% goes into the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "scaler = preprocessing.StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "y_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Fitting the model\n",
    "Now we're ready to fit a ridge regression model and select $\\lambda$ via cross-validation. Recall that the aim is to be able to predict a player's salary based on all of the other variables in the dataset.\n",
    "\n",
    "We are going to use the RidgeCV class. The optional inputs include the following:\n",
    "- alphas: The values of the regularization parameter $\\lambda$ to try. The default is (0.1, 1.0, 10.0).\n",
    "- fit_intercept: Whether to include an intercept or not. The default is True.\n",
    "- normalize: Whether to scale the features so they have unit norm. The default is False.\n",
    "- cv: Determines the cross-validation splitting strategy. The default is to use leave-one-out cross-validation.\n",
    "\n",
    "Note that sklearn uses the following version of the ridge regression objective function:\n",
    "$$\\min_\\beta \\|y-X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "lambda_vals = [10**k for k in range(-5, 6)]\n",
    "ridge = RidgeCV(alphas=lambda_vals, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RidgeCV has many of the same methods as KNeighborsClassifier. Its methods include:\n",
    "- fit(X, y[, sample_weight]): Fit the ridge regression model\n",
    "- get_params(): Get parameters for this estimator.\n",
    "- predict(X): Predict using the linear model\n",
    "- score(X, y[, sample_weight]):\tReturns the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "It also has several attributes that will be of interest:\n",
    "- coef_: Estimated coefficients\n",
    "- intercept_: The estimated intercept\n",
    "- alpha_: The estimated regularization parameter $\\lambda$\n",
    "\n",
    "Let's fit the model and then get the chosen $\\lambda$ and the estimated coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)\n",
    "print('Estimated lambda =', ridge.alpha_)\n",
    "print('Estimated coefficients =', ridge.coef_)\n",
    "print('Estimated intercept =', ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Predicting and computing the MSE\n",
    "Finally, let's use the predict method and then compute the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = ridge.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can compare the above result to results using (1) The null model; and (2) A linear regression model. For this we don't need to use cross-validation, so we can just use the Ridge class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to the null model\n",
    "from sklearn.linear_model import Ridge\n",
    "null_model = Ridge(alpha=10**10, normalize=True)\n",
    "null_model.fit(X_train, y_train)\n",
    "y_pred = null_model.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to linear regression\n",
    "from sklearn.linear_model import Ridge\n",
    "linModel = Ridge(alpha=0, normalize=True)\n",
    "linModel.fit(X_train, y_train)\n",
    "y_pred = linModel.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Alternatively, we can use the LinearRegression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_model = LinearRegression(normalize=True)\n",
    "lin_model.fit(X_train, y_train)\n",
    "y_pred = lin_model.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE on the test set when using our ridge regression model is smaller than when using a linear regression model or the null model, which is what we would expect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
