{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luke Waninger\n",
    "#### 25 May 2018 \n",
    "#### Homework 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from H7_source import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "exp = np.exp\n",
    "na = np.newaxis\n",
    "norm = np.linalg.norm\n",
    "ident = np.identity\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute the gradient $\\triangledown F(\\alpha)$ of $F$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(k, y, beta, l, h=0.5):\n",
    "    n, d = k.shape\n",
    "    lg = np.zeros([n, d])\n",
    "\n",
    "    yk = y *(k @ beta)\n",
    "    mask = np.abs(1 - yk)\n",
    "\n",
    "    lg[mask <= h] = ((1/(2*h)) * ((1 + h-yk)[:, na]) * (-y[:, na] * k))[mask <= h]\n",
    "    lg[yk < 1-h]  = (-y[:, na] * k)[yk < 1-h]\n",
    "\n",
    "    return np.array(np.sum(lg, axis=0)/n + 2*l*beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function $\\texttt{computegram}$, $\\texttt{kerneleval}$\n",
    "I decided to encapsulate these functions into one class each for radial and polynomial kernels for readability and code reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_radialrbf(Kernel):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'rbf({self.sigma})'\n",
    "\n",
    "    def compute(self, x, xp=None):\n",
    "        sigma = self.sigma\n",
    "        xp = x if xp is None else xp\n",
    "\n",
    "        def norm(mat):\n",
    "            return np.linalg.norm(mat, axis=1)\n",
    "\n",
    "        return exp(-1 / (2 * sigma ** 2) * ((norm(x) ** 2)[:, na] + (norm(xp) ** 2)[na, :] - 2 * (x @ xp.T)))\n",
    "\n",
    "\n",
    "class k_polynomial(Kernel):\n",
    "    def __init__(self, degree, b=1.):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.b = b\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'polynomial({self.degree})'\n",
    "\n",
    "    def compute(self, x, xp=None):\n",
    "        xp = x if xp is None else xp\n",
    "\n",
    "        return (x @ xp.T + self.b) ** self.degree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### consider the Digits dataset, download and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "scalar  = StandardScaler().fit(x_train)\n",
    "x_train = scalar.transform(x_train)\n",
    "x_test  = scalar.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function $\\texttt{mysvm}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVM(Estimator):\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.gradient = gradient\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'SVM(kernel={self.kernel})'\n",
    "    \n",
    "    def fgrad(self, k, y, l, eta=1., max_iter=100):\n",
    "        n, d  = k.shape\n",
    "        b0    = np.zeros(d)\n",
    "        theta = np.copy(b0)\n",
    "        grad  = self.gradient(k, y, b0, l)\n",
    "\n",
    "        i = 0\n",
    "        while i < max_iter and not np.isclose(0, eta):\n",
    "            eta = backtracking(k, y, n, b0, l, eta, self.gradient, self.objective)\n",
    "\n",
    "            b1 = theta - eta*grad\n",
    "            theta = b1 + (i/(i+3))*(b1-b0)\n",
    "            grad  = self.gradient(k, y, theta, l)\n",
    "            b0 = b1\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        return b0\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective(k, y, l, beta, h=0.5):\n",
    "        n, d = k.shape\n",
    "        loss = np.zeros(n)\n",
    "        yk = y * (k @ beta)\n",
    "        mask = np.abs(1 - yk)\n",
    "\n",
    "        loss[mask <= h] = ((1 + h-yk)**2 / (4*h))[mask <= h]\n",
    "        loss[yk < 1-h] = (1 - yk)[yk < 1-h]\n",
    "\n",
    "        return np.sum(loss)/n + l*norm(beta)**2\n",
    "\n",
    "    def predict(self, kp, beta):\n",
    "        return [1 if ki @ beta.T > 0 else -1 for ki in kp]\n",
    "    \n",
    "    def predict_proba(self, kp, beta):\n",
    "        return [ki @ beta.T for ki in kp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26265062e+12,  7.47363597e+05, -1.93224148e+08, ...,\n",
       "        -1.38232013e+03,  4.39391201e+06, -4.66542513e-05],\n",
       "       [ 7.47363597e+05,  1.48105710e+11,  1.92712053e+06, ...,\n",
       "         7.61470741e+07,  6.32965585e+04,  2.78854033e+10],\n",
       "       [-1.93224148e+08,  1.92712053e+06,  2.51973127e+11, ...,\n",
       "         1.44519975e+08,  9.44285484e+01,  9.80161028e+06],\n",
       "       ...,\n",
       "       [-1.38232013e+03,  7.61470741e+07,  1.44519975e+08, ...,\n",
       "         1.76237741e+11,  3.84046216e+04,  3.65261096e-01],\n",
       "       [ 4.39391201e+06,  6.32965585e+04,  9.44285484e+01, ...,\n",
       "         3.84046216e+04,  3.46155188e+10, -5.44572989e-01],\n",
       "       [-4.66542513e-05,  2.78854033e+10,  9.80161028e+06, ...,\n",
       "         3.65261096e-01, -5.44572989e-01,  1.27256605e+11]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_polynomial(7).compute(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:08.831393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.68520820e+09, 5.61695623e+08, 2.27322493e+09, ...,\n",
       "       1.55505242e+08, 2.63093661e+08, 2.58941755e+08])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernl = k_polynomial(7)\n",
    "timeit(lambda: MySVM(kernl).fgrad(kernl.compute(x_train), y_train, 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running one vs rest with a polynomial kernel of degree 7 gives a horrible validation error: $\\approx$ 0.518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#OVR(MySVM(kernl), n_jobs=-1).fit(x_train, y_train, x_test, y_test, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using cross-validation we see a slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(x, y, x_test, y_test, estimator, eargs, nfolds=3):\n",
    "    pbar = track_bar(track_total(estimator, eargs), desc=f'{nfolds}-Fold CV: {estimator}, reg: {eargs}')\n",
    "    step = int(x.shape[0]/nfolds)\n",
    "    \n",
    "    for arg in eargs:\n",
    "        tidx = np.random.choice(np.arange(len(y)), 2*step)\n",
    "        vidx = list(set(np.arange(len(y))) - set(tidx))\n",
    "        xa, ya, xva, yva = x[tidx, :], y[tidx], x[vidx, :], y[vidx]\n",
    "        estimator = estimator.fit(xa, ya, xva, yva, arg, pbar)\n",
    "        \n",
    "    [pbar.put(f) for f in [1, 'END_FLAG']]\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3-Fold CV: <OVR(estimator=SVM(kernel=polynomial(7)) err=0.0)>, reg: [0.001   0.25075 0.5005  0.75025 1.     ]: 100%|██████████| 50/50 [01:05<00:00,  1.39it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OVR(estimator=SVM(kernel=polynomial(7)) err=0.42966506201657173)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0VR lambda=0.001\n",
      "1VR lambda=0.5005\n",
      "2VR lambda=0.001\n",
      "3VR lambda=0.001\n",
      "4VR lambda=1.0\n",
      "5VR lambda=0.001\n",
      "6VR lambda=0.001\n",
      "7VR lambda=0.001\n",
      "8VR lambda=0.001\n",
      "9VR lambda=0.001\n"
     ]
    }
   ],
   "source": [
    "ovr = OVR(MySVM(kernl), n_jobs=-1)\n",
    "cv(x_train, y_train, x_test, y_test, ovr, np.linspace(.001, 1., 5))\n",
    "print(ovr.kl_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It quickly becomes clear the 7-degree polynomial kernel is a horrible choice. Below, I run a series of OVO polynomial and radial kernels that all show much better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\n",
    "    k_polynomial(1),\n",
    "    k_polynomial(3),\n",
    "    k_polynomial(5),\n",
    "    k_radialrbf(1),\n",
    "    k_radialrbf(5),\n",
    "    k_radialrbf(10)\n",
    "]\n",
    "\n",
    "for kernl in kernels:\n",
    "    ovr, eargs = OVR(MySVM(kernl), n_jobs=-1), np.linspace(.001, 1., 5)\n",
    "    cv(x_train, y_train, x_test, y_test, ovr, eargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
